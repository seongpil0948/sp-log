## 도움이 될만한 명령어들
#TODO: 정리 

### ip link

```bash
# list an modify interface on the host
ip link
# add bridge
ip link add br0 type bridge
# up bridge
ip link set br0 up
# ip addresses assigned to the interfaces
ip -c add
# or 
ip addr # or ip a


```
이 명령어들 즉, ip 커맨드는 시스템이 restart 되면 리셋됩니다. 테스트 용도로 사용하기 좋습니다.
`netplan get` 명령어를 통해 현재 설정을 확인할 수 있습니다.
영구변경을 원한다면 아래 파일을 수정하고
```sh
cat /etc/netplan/00-installer-config.yaml
```
예시임, 네트워크 인터페이스, IP 주소, DNS 서버, 라우팅 정보 등을 구성합니다.
```yaml
network:
  ethernets:
    enp0s3:
      # DHCP를 통해 IPv4 주소를 자동으로 할당받도록 설정합니다.
      dhcp4: true
      # 이 인터페이스에 할당할 IP 주소를 지정합니다
      # addresses:
        # - 10.12.34.56/24
  version: 2
``` 
이후 아래 명령어를 통해 반영합니다.
```sh
# for dry run
sudo netplan try

# force apply
sudo netplan apply
```
> 참고로 `netplan` 은 `networkd` 를 위한 설정 도구입니다. `networkd` 는 `systemd` 의 일부입니다.

하지만 명령어를 통해 설정을 변경하는것은 잠재적 위험이 있습니다.
시스템 시작시 `/etc/netplan` 경로를 alphabetical order 로 읽기 때문에
`/etc/netplan/01-network-manager-all.yaml` 파일을 작성하면 덮어씌워집니다.

아래 파일을 수정하면 됩니다.

```yaml
# /etc/netplan/01-network-manager-all.yaml
network:
  ethernets:
    enp0s3:
      dhcp4: false
      addresses:
        - 10.12.34.56/24
        - 192.168.0.103/24
        - fe80::9640:c9ff:fe26:370c/64
      nameservers:
        # resolve ip address from domain name that is not in /etc/hosts
        # query to below dns servers 
        addresses:
          - 8.8.4.4
          - 8.8.8.8
      routes:
        # 192.168.0.0/24 네트워크로 향하는 트래픽을 10.0.0.100 게이트웨이를 통해 라우팅합니다.
        - to: 192.168.0.0/24
          via: 10.0.0.100
        # 다른 네트워크로 향하는 트래픽은 10.0.0.1 게이트웨이를 통해 라우팅됩니다.
        - to: default
          via: 10.0.0.1
  version: 2
``` 
반영 후 `ip route `의 `routes` 필드 반영 여부를 확인
` resolvectl status` 명령어를 통해 `nameservers` 필드 반영 여부를 확인
글로벌 적용은 `/etc/resolv.conf` 파일을 수정하면 됩니다.
```
# /etc/resolv.conf
[Resolve]
DNS=8.8.8.8 1.1.1.1
```
이후 `systemd-resolved` 서비스를 재시작합니다.
```sh 
sudo systemctl restart systemd-resolved
resolvectl dns
```
더 많은 예제는 `/usr/share/doc/netplan/examples` 에서 확인할 수 있습니다.
영구적 적용을 위해서는 `/etc/network/interfaces` 파일을 수정해야 합니다.  

#TODO: Linking basic Network link



## Network Namespace
* 네트워크 스택을 격리하는 가상화 기술입니다.
* `ip netns list` 명령어를 통해 네임스페이스를 확인할 수 있습니다.
OCI 이미지를 빌드할 때 Command 키워드는 컨테이너내에서 첫 번째로 실행할 프로세스를 지정할 수 있습니다.
예로 `docker run -it ubuntu bash` 명령어를 실행하면 *bash* 프로세스가 실행됩니다.
즉 컨테이너 내에서 `ps aux` 명령어를 실행하면 PID=1 로서 *bash* 가 실행됩니다.
하지만 host 에서 `ps aux` 명령어를 실행하면 PID=1가 아닌 다른 프로세스 ID가 출력됩니다.
이는 컨테이너가 생성될 때 컨테이너의 네임스페이스를 생성하고, 네임스페이스 내에서 프로세스를 실행하기 때문입니다.

호스트 네트워크에서 컨테이너의 어떠한 네트워크도 보이지 않습니다.
왜냐하면 컨테이너는 네임스페이스는 별도의 이더넷 인터페이스(vth)를 생성하여
네트워크 인터페이스, 라우팅 테이블, ARP 테이블, IP 룰, IP 주소 공간 등을 격리하기 때문입니다.

### Veth(이더넷 인터페이스)
veth는 쌍으로 만들어지며 네트워크 네임스페이스들을 터널로서 연결하거나, 물리 디바이스와 다른 네트워크 네임스페이스의 장비를 연결하는 용도로 사용할 수 있습니다.


생성해 볼까요
```bash
# create veth pair
ip link add veth-red type veth peer name veth-blue
# set veth-red to red namespace
ip link set veth-red netns red
# set veth-blue to blue namespace
ip link set veth-blue netns blue
```
> 참고로 veth-red 와 veth-blue 는 같은 물리적 네트워크 인터페이스를 공유합니다.

 빨간색 네임스페이스에 ip 192.168.15.1을, 파란색 네임스페이스에 ip 192.168.15.2를 할당 하겠습니다.
```bash
ip -n red addr add 192.168.15.1 dev veth-red
ip -n blue addr add 192.168.15.2 dev veth-blue

# activate vth
ip -n red link set veth-red up
ip -n blue link set veth-blue up
```

두 네임스페이스가 서로 통신할 수 있습니다.
```bash
# ping from red to blue
ip netns exec red ping 192.168.15.2
# ping from blue to red
ip netns exec blue ping 192.168.15.1

# check arp table of red(blue address: 192.168.15.2)
ip netns exec red arp -n
# check arp table of blue(red address: 192.168.15.1)
ip netns exec blue arp -n
```

그리고 이 만들어진 vth들을 외부 네트워크와 연결하고 싶다면 어떻게 해야할까요?
물리적 네트워크와 같이 동작하게 하려면 스위치를 사용해야 합니다.
가상 스위치, 브릿지를 만들어 볼까요?
```bash
# create bridge
ip link add v-net-0 type bridge
# activate bridge
ip link set dev v-net-0 up
ip link
```
이제 호스트에 연결될 인터페이스가 보입니다.
이건 우리 호스트와 네임스페이스를 연결하는 인터페이스라고 생각하면 됩니다.

이전에 직접 링크한 veth-blue, veth-red 를 지우겠습니다.
```bash
ip -n red link del veth-red 
ip -n blue link del veth-blue
```

그리고 다시 브릿지에 연결하겠습니다.
```bash
ip link add veth-red type veth peer name veth-red-br
ip link add veth-blue type veth peer name veth-blue-br

ip link set veth-red netns red
ip link set veth-red-br master v-net-0

ip link set veth-blue netns blue
ip link set veth-blue-br master v-net-0
```
다시 각각 ip를 할당합니다.
```bash
ip -n red addr add 192.168.15.1 dev veth-red
ip -n blue addr add 912.168.15.2 dev veth-blue
# activate vth
ip -n red link set veth-red up
ip -n blue link set veth-blue up
```
이렇게 브릿지를 통해 네임스페이스간 네트워크가 생성되었습니다.

호스트와 네임스페이스가 통신이 가능할까요?
```bash
ping 192.168.15.1
```
정답은 No 입니다.

브릿지(네임스페이스)는 호스트 네트워크와 연결되지 않아있습니다. 
호스트와 연결하려면 어떻게 해야할까요? 
vth는 같은 물리적 네트워크 인터페이스를 공유합니다. 우리는 그저 IP 주소를 할당하면 됩니다.  
이를 통해 네임스페이스에 접근할 수 있습니다.
```bash
ip addr add 192.168.15.5/24 dev v-net-0
```
이제 핑이 잘될겁니다 큭큭
하지만 이 네트워크는 아직 private(*192.168.15.0*) 입니다. 어떠한 인터페이스도 외부 네트워크와 연결되어 있지 않습니다.
오직 eth0(ethernet, 192.168.1.2) 인터페이스만 외부 네트워크와 연결되어 있습니다.

만약 외부 네트워크(LAN)과 연결 하고 싶다면 어떻게 해야할까요?
우린 이전 네트워크 *192.168.15.0/24* 를 eth0(*192.168.1.2*) 를 통해 LAN(*192.168.1.0*) 네트워크 대역에서 
외부 네트워크 eth1(192.168.1.3) 대역 네트워크와 연결할 수 있습니다.
```bash
# 브릿지를 통해 LAN 네트워크와 연결
ip netns exec blue ip route add 192.168.1.0/24 via 192.168.15.5
```
이제 우리는 eth1(192.168.1.3) 와 통신이 가능할까요?
`ip netns exec blue ping 192.168.1.3` 명령어를 실행해보세요.
안됩니다. 우리 blue는 private network에 있기 때문입니다.
외부로 패킷은 내보낼 수 있지만, 외부에서 패킷을 받을 수 없습니다.
외부에서 패킷을 받기 위해서는 Gateway network 내에서 NAT(Network Address Translation)을 활성화 해야합니다.

```bash
# MASQUERADE: NAT
# 192.168.15.0/24: This specifies the source of the packets.
# MASQUERADE: source IP of the packets is not known until the packets are routed.
iptables -t nat -A POSTROUTING -s 192.168.15.0/24 -j MASQUERADE
``` 
LAN 네트워크 연결이 되었습니다.
마지막으로 우리는 네임스페이스 blue가 인터넷에 연결되기를 원합니다. 
> 가정1: LAN이 인터넷에 연결되어 있다
> 가정2: blue namespace 의 80 포트로 웹서버가 실행되고 있다.

물론 직접 연결로 가능하겠지만 우리는 iptables를 사용해 보겠습니다.

```bash
# This matches destination port 80, which is typically used for HTTP traffic.
# --to-destination 192.168.15.2:80: This is the new destination of the packet. In this case, it's IP address 192.168.15.2 on port 80.
# -j DNAT: This is the target of the rule. DNAT is a form of Destination NAT that alters the destination IP address of the packet to route it to a specified address.
#  DNAT는 패킷의 대상 IP 주소를 변경하여 지정된 주소로 라우팅하는 대상 NAT의 한 형태입니다.
iptables -t nat -A PREROUTING --dport 80 --to-destination 192.168.15.2:80 -j DNAT
```
결론: 그래도.. 한번에 양방향 통신이 가능한게 어디냐.. ㅠㅠ


### ARP(Address Resolution Protocol)
ARP는 IP 주소를 물리적 네트워크 주소(MAC)로 매핑하는 프로토콜입니다.

## Bridge
브리지는 두 개의 네트워크를 연결하는 장치입니다. 브리지는 두 개의 네트워크 인터페이스를 가지고 있습니다.

### Gateway
집에서 인터넷을 사용할 때 라우터는 홈 네트워크에서 세계의 다른 네트워크로 데이터를 전송하는 데 도움이 되기 때문에 게이트웨이입니다.


## Refer
* https://www.44bits.io/ko/post/container-network-2-ip-command-and-network-namespace
* 



# Tasks

## 192.168.0.100:8083 에대한 경로를 분석해라

먼저 방화벽 규칙중 tcp, udp 프로토콜에 대한 규칙을 보자
```sh
# sudo iptables -L"


Chain DOCKER (1 references)
target     prot opt source               destination
ACCEPT     tcp  --  anywhere             172.17.0.2           tcp dpt:http


# 172.17.0.2 ip 를 가진 NIC이 들어오는 tcp 패킷을 허용한다.
```
이외에 딱히 DROP, QUEUE, SNAT -> DNAT 되는 규칙은 없다.
172.17.0.2 에 해당하는 주요 라우팅 테이블, NIC 주소를 살펴보자

```sh
# ip route 

# 다른 규칙과 일치하지 않는 모든 트래픽에 대한 기본 경로를 지정합니다.
# 패킷은 게이트웨이 192.168.0.1을 통해 인터페이스 enp2s0f0로 전송됩니다.
# 이 경로는 DHCP(동적 호스트 구성 프로토콜)를 통해 학습되었습니다.
# 나가는 트래픽의 소스 IP 주소는 192.168.0.100입니다.
# 경로의 메트릭(우선 순위)은 100입니다.
default via 192.168.0.1 dev enp2s0f0 proto dhcp src 192.168.0.100 metric 100
# 로컬 네트워크(192.168.0.0/24)에 직접 enp2s0f0를 통해 액세스합니다.
192.168.0.0/24 dev enp2s0f0 proto kernel scope link src 192.168.0.100

# Docker 네트워크(172.17.0.0/16)에 직접 docker0 인터페이스를 통해 액세스합니다.
# 이 경로는 커널(수동으로 추가되지 않음)에 의해 관리됩니다.
# 링크 로컬 경로이며, 즉 대상이 동일한 네트워크에 직접 연결되어 있음을 의미합니다.
# 이 네트워크로의 트래픽의 소스 IP 주소는 172.17.0.1입니다.
172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1

# ip addr
2: enp2s0f0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000
    link/ether 94:40:c9:26:3a:2c brd ff:ff:ff:ff:ff:ff
    inet 192.168.0.100/24 brd 192.168.0.255 scope global dynamic enp2s0f0
       valid_lft 83615sec preferred_lft 83615sec
    inet6 fe80::9640:c9ff:fe26:3a2c/64 scope link
       valid_lft forever preferred_lft forever
```
default 가 이더넷 인터페이스(enp2s0f0)로 설정되어 있지만 방화벽 필터에 따라 
192.168.0.0/24 요청이 올경우 docker0 인터페이스로 이동하는 것을 확인했습니다.


