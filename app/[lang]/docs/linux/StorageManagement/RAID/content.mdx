# What is RAID?
RAID는 Redundant Array of Independent Disks의 약자로, 여러 개의 하드디스크를 하나로 묶어서 사용하는 기술이다.   
만약 Disk A (1TB)와 Disk B (1TB)가 있고 우리는 1.5TB의 저장공간이 필요하다.   
하지만 어떠한 기기도 이러한 요구사항을 충족시키지 못하는 상황이다.   
이때 RAID 기술을 사용하면 Disk A와 Disk B를 묶어서 2TB의 저장공간을 만들 수 있다.  
위 묶인 번들을 RAID Array라고 부르며 추가 데이터를 목적으로 할 뿐 아니라, RAID Level에 따라 데이터의 안정성, 가용성을 높일 수 있다.

시작하기전에 범용적으로 사용되는 패리티코드와 해밍코드에 대해 알아보자.
### 패리티 코드(Parity Code)
> 패리티 비트는 오류를 검출하기 위해 존재한다.
* 하나의 문자 또는 수는 8비트로 표현된다. 
* 8비트는 ASCII Code(7비트)와 Parity Code(1비트)로 구성되어 있다. 
* Linux storage 관리에서 추가 정보, 즉 재구축에 사용할 수 있는 작은 백업으로 생각할 수 있습니다.
* 패리티 비트의 위치는 가장 끝에 있다.
* ex) A(65) : 1000001 + 패리티 비트
  * 홀수 패리티: 1000001+1
  * 짝수 패리티: 1000001 + 0
1. 홀수 패리티 코드
홀수 패리티 코드에서는 데이터에 포함된 1의 개수가 홀수가 되도록 패리티 비트를 설정합니다. 예를 들어, 데이터가 1001인 경우 패리티 비트는 1을 추가하여 10011이 됩니다.

만약 데이터 전송 과정에서 1비트 오류가 발생하여 10011이 10111로 변형된다면, 수신 측에서는 1의 개수가 짝수가 되었음을 확인하고 오류가 발생했다는 것을 알 수 있습니다.
2. 짝수 패리티 코드
마찬가지로 짝수 패리티 코드는 데이터에 포함된 1의 개수가 짝수가 되도록 패리티 비트를 설정합니다. 예를 들어, 데이터가 1001인 경우 패리티 비트는 0을 추가하여 10010이 됩니다.  

--- 

**주의**  
패리티 코드는 1비트 오류만 검출할 수 있으며, 2비트 이상의 오류는 검출하지 못합니다. 또한, 패리티 비트 자체에 오류가 발생하는 경우에도 오류를 감지하지 못할 수 있습니다.

### 해밍 코드(Hamming Code)
패리티 코드는 오류를 검출만 가능했지만, 해밍 코드는 오류 발생 위치까지 파악하여 데이터를 복구할 수 있습니다.  
예시  
1. 데이터 비트: 1001
2. 체크 비트 2개 추가: 100111
3. 체크 비트 1: **2의 거듭제곱 번째 비트** (1, 2, 4, 8, ...)의 1의 개수가 홀수인지 짝수인지 확인 
4. 체크 비트 2: **2의 거듭제곱 - 1 번째 비트**(3, 5, 7, 11, ...)의 1의 개수가 홀수인지 짝수인지 확인 

## RAID Level
RAID 레벨(0~6)은 데이터를 관리하는 방식을 의미한다.  ( `RAID1`, `RAID5`, `RAID6` 등)
1. RAID0: 
   1. 데이터를 균등히 나누어 저장하는 방식 
   2. 하드디스크의 용량을 합친 용량만큼 사용할 수 있다. 
   3. 하나의 디스크가 고장나면 모든 데이터가 손실된다.
   4. stripe set  또는  striped volume이라고도 불린다.
   5. [패리티 비트](https://en.wikipedia.org/wiki/Parity_bit), 중복성 및 [내결함성](https://en.wikipedia.org/wiki/Fault_tolerance)이 없습니다.
2. RAID1
   1. 데이터를 두 개 이상의 디스크에 동시에 저장하는 방식
   2. 쓰기 성능보다 읽기 속도와 안정성에 강하다. 
   3. 하나의 디스크가 고장나도 다른 디스크에 데이터가 남아있어 복구가 가능하다. 
   4. 하드디스크의 용량은 하나의 디스크 용량만큼 사용할 수 있다.
   5. 패리티 비트 미사용
3. RAID2: 
   1. 데이터를 나누어(순환 spin) 저장
   2. ECC(오류 검출 및 수정)을 위한 [Hamming code](https://en.wikipedia.org/wiki/Hamming_code)를 추가로 저장하는 방식으로, 패리티 비트를 사용하지 않는다.
   3. 속도와 안정성을 둘 다 고려한 방식이다. 
   4. 하나의 디스크가 고장나도 ECC 코드를 이용하여 데이터를 복구할 수 있다. 
   5. 하드디스크의 용량은 하나의 디스크 용량만큼 사용할 수 있다.  
4. RAID3: 
   1. 데이터를 byte 단위로 스트라이프(여러 디스크에 걸쳐) 블록을 저장
   2. 일반적으로 여러 요청을 동시에 처리할 수 없습니다. 이는 단일 데이터 블록이 정의에 따라 세트의 모든 구성원에 분산되고 각 디스크의 동일한 물리적 위치에 상주하기 때문에 발생합니다.
   3. 실제로는 사용되지 않는다.
   4. 전용 패리티 디스크를 사용
   5. 하드디스크의 용량은 하나의 디스크 용량만큼 사용할 수 있다.  
5. RAID4: 
   1. block 단위로 스트라이프 데이터를 저장
   2. 전용 패리티 디스크를 사용
   3. random read/write 성능이 높다. (블록단위로 디스크에 저장되기 때문!)
   4. 하드디스크의 용량은 하나의 디스크 용량만큼 사용할 수 있다.  
6. RAID5: 
   1. 데이터를 나누어 저장하고, 
   2. 패리티 정보(8bit + 1bit(parity) -> 데이터 전송 성공여부 )를 함께 여러 디스크에 분산 저장하는 방식 
      1. 
      2. 하나의 디스크가 고장나도 패리티 정보를 이용하여 데이터를 복구할 수 있다. (하나만 이어야함 ㅋㅋ)
   3. 단일 드라이브에 오류가 발생하면 데이터가 손실되지 않도록 분산 패리티에서 후속 읽기를 계산할 수 있습니다. 
   4. 하나의 디스크가 손실되더라도 나머지 두 디스크에는 여전히 일부 패리티가 남아 있습니다. 그리고 이러한 디스크의 패리티를 사용하여 첫 번째 디스크에서 손실된 데이터를 재구축할 수 있습니다.
   5. RAID 5에는 디스크가 3개 이상 필요합니다.
   6. 1테라바이트의 디스크 3개가 있으면 1테라바이트의 디스크가 2테라바이트의 사용 가능한 공간을 얻게 됩니다. 
      1. 패리티 코드는 각 디스크의 0.33TB 씩 저장됩니다.
      2.  1TB의 디스크 10개가 있다면 9TB의 사용 가능한 공간을 얻게 됩니다.
7. RAID6: 
   1. RAID5와 비슷하지만, 패리티 블록을 두 개의 디바이스에 안정성을 높인 방식이다. 
   2. 즉 디스크가 4개 이상 필요하다.
   3. 하드디스크의 용량은 하나의 디스크 용량만큼 사용할 수 있다.
8. RAID10 (1+0)
   1. 레벨 1과 레벨 0을 결합한 방식
   2. 레벨 0에서는 2개의 1테라바이트 디스크가 2테라바이트의 array로 이어질 수 있습니다.
   3. 레벨1을 적용하여 2테라바이트의 array가 다른 2테라바이트의 array와 동일한 데이터를 저장하도록 합니다.
   4. 레벨 1과 레벨 0을 결합하여 적은양의 증분효과를 얻지만 안정성을 높일 수 있습니다.
위처럼 각 레벨의 특성을 고려하여 설계하여 사용해야한다.

## mdadm
mdadm(manage Multiple Devices devices aka Linux Software RAID)는 리눅스에서 소프트웨어 레벨에서 RAID를 관리하는데 사용되는 도구이다.  

```bash
mdadm [mode] <raiddevice> [options] <component-devices>
```
###  Modes
- --create: 새로운 RAID array를 생성합니다.
- --assemble: 이전에 생성된 RAID array를 재조립합니다.
- --build: RAID array를 생성하지 않고, 디스크를 검사합니다.
- --grow: RAID array의 크기를 변경합니다.
- --manage: RAID array의 관리를 합니다.
- --monitor: RAID array의 모니터링을 합니다.
- --misc: 기타 명령어를 실행합니다.
- --incremental: RAID array의 증분을 실행합니다.

### Options
- --level: RAID 레벨을 설정합니다.
- --raid-devices: RAID array에 사용할 디스크의 개수를 설정합니다.
- --spare-devices: RAID array에 사용할 스페어 디스크의 개수를 설정합니다.
- --bitmap: RAID array에 사용할 비트맵을 설정합니다.
- --name: RAID array의 이름을 설정합니다.
- --homehost: RAID array의 호스트 이름을 설정합니다.
- --uuid: RAID array의 UUID를 설정합니다.


Let's jump to the commands
---
만약 이전시에 LVM을 사용했다면 /dev/vdc, /dev/vdd, /dev/vde 디스크를 release 해주자.  
```bash
$ sudo vgremove --force my_volume
$ sudo pvremove /dev/vdc /dev/vdd /dev/vde
```
create a `/dev/md0` device with RAID0 level and 3 disks
```bash
$ sudo mdadm --create --verbose /dev/md0 --level=0 --raid-devices=3 /dev/vdc /dev/vdd /dev/vde
```
합쳐진 디스크는 일반 디스크와 동일하게 사용할 수 있다.  
```bash
$ sudo mkfs.ext4 /dev/md0

# stop and deactivate the RAID device
$ sudo mdadm --stop /dev/md0
```
그러나 Linux가 부팅되면 모든 장치의 슈퍼 블록(super block)을 검색합니다.  
간단히 말해서, 장치에 속해 있는 정보가 있는지 확인하려고 시도합니다.  
이경우 시스템에 의해 `/dev/md1`, `/dev/md2`, `/dev/md3` 등의 장치가 생성될 수 있습니다.  
이를 방지하기 위해 생성시 `--zero-superblock` 옵션을 사용하여 슈퍼블록을 제거할 수 있습니다.  
```bash
$ sudo mdadm --zero-superblock /dev/vdc /dev/vdd /dev/vde
```
이제 `RAID1`과 같이 미러링 데이터에 대해 살펴보자.  
어느 날 디스크가 오작동하고 모든 데이터가 손실되었다.   
다행히 다른 하나는 여전히 잘 작동하므로 데이터가 그대로 남아있다.  
make `RAID1`
```bash
$ sudo mdadm --create --verbose /dev/md0 --level=1 --raid-devices=2 /dev/vdc /dev/vdd --spare-devices=1 /dev/vde
```
운영체제는 특정 디스크가 손실되었을 때, --spare-devices 인 /dev/vde를 사용하여 데이터를 복구할 수 있다.  
```bash
# clean up
$ sudo mdadm --stop /dev/md0
$ sudo mdadm --zero-superblock /dev/vdc /dev/vdd /dev/vde
```

만약 두디스크 모두 손실될 위험이 있다면 디스크를 추가해주자
```bash
$ sudo mdadm --create --verbose /dev/md0 --level=1 --raid-devices=3 /dev/vdc /dev/vdd 
$ sudo mdadm --manage /dev/md0 --add /dev/vde
# if you want to remove a disk
$ sudo mdadm --manage /dev/md0 --remove /dev/vde
```
생성된 RAID 디스크는 다음과 같이 확인할 수 있다.
```bash
$ cat /proc/mdstat
# RAID 타입을 확인할 수 있다.
Personalities : [linear] [multipath] [raid0] [raid1] [raid6] [raid5] [raid4] [raid10]
# RAID 네임과 디바이스를 확인할 수 있다.
md0 : active raid1 vde[2] vdd[1] vdc[0]
      1047552 blocks super 1.2 [3/2] [UU_]
      bitmap: 1/1 pages [4KB], 65536KB chunk
```


# Practice

### Create a level 1 RAID array, at `/dev/md0`, with two devices: `/dev/vdc` and `/dev/vdd`.

```bash
$ sudo mdadm --create --level=1 /dev/md0  --raid-devices=2 /dev/vdc /dev/vdd
mdadm: Note: this array has metadata at the start and
    may not be suitable as a boot device.  If you plan to
    store '/boot' on this device please ensure that
    your boot-loader understands md/v1.x metadata, or use
    --metadata=0.90
Continue creating array? yes              
mdadm: Defaulting to version 1.2 metadata
mdadm: array /dev/md0 started.

$ cat /proc/mdstat 
Personalities : [raid1] 
md0 : active raid1 vdd[1] vdc[0]
      1046528 blocks super 1.2 [2/2] [UU]
```

### Add another device, /dev/vde, to the previously created array, /dev/md0.
   
```bash
$ sudo mdadm --manage /dev/md0 --add /dev/vde 
mdadm: added /dev/vde
```












## Reference
- [RAID](https://en.wikipedia.org/wiki/RAID)
- [RAID levels](https://en.wikipedia.org/wiki/Standard_RAID_levels)
- [해밍, 패리티](https://amber-chaeeunk.tistory.com/122)
- [해밍](https://www.youtube.com/watch?v=X8jsijhllIA)